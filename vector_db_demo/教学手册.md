# LangChain 向量数据库与检索器 - 教学手册

本教学手册旨在帮助初学者理解 LangChain 向量数据库和检索器的基本概念和实现步骤。本项目使用了 LangChain 0.3 版本，基于本地嵌入模型和内部部署的 LLM 服务。

## 一、基础概念介绍

### 1.1 什么是向量数据库？

向量数据库是一种专门用于存储和检索高维向量数据的数据库系统。在 LLM 应用中，文本内容会被转换为高维向量（嵌入），这些向量捕捉了文本的语义信息。向量数据库可以高效地进行相似性搜索，找出与查询向量最相似的向量集合。

### 1.2 什么是检索增强生成（RAG）？

检索增强生成（Retrieval-Augmented Generation，RAG）是将检索系统与生成 AI 模型相结合的技术。当用户提出问题时，系统首先检索与问题相关的文档或信息，然后将这些检索到的信息作为上下文提供给语言模型，使模型能够生成更准确、更相关的回答。

RAG 的工作流程：
1. 用户提问
2. 系统将问题转换为查询向量
3. 在向量数据库中检索相关文档
4. 将检索到的文档与原始问题一起提供给语言模型
5. 语言模型生成回答

### 1.3 为什么需要本地嵌入模型？

使用本地嵌入模型而非第三方在线服务有几个优势：
- **数据隐私**：敏感数据不会发送到外部服务
- **成本控制**：避免 API 调用费用
- **离线工作**：不依赖网络连接
- **自定义控制**：可以根据需要选择或微调嵌入模型

## 二、项目技术栈解析

### 2.1 核心技术组件

1. **LangChain 0.3**：提供了构建 LLM 应用的框架
   - `langchain-core`: 核心接口和基类
   - `langchain-community`: 社区实现的各种集成

2. **Sentence Transformers**：本地嵌入模型
   - 使用了预训练的多语言模型 `paraphrase-multilingual-MiniLM-L12-v2`
   - 支持中文和多语言嵌入

3. **FAISS (Facebook AI Similarity Search)**：高效的向量搜索库
   - 支持对高维向量进行相似度搜索
   - 提供了多种索引类型，适用于不同规模的数据

4. **自定义 LLM API**：连接内部部署的语言模型

### 2.2 项目模块功能

1. **本地嵌入模型 (LocalEmbeddings)**
   - 使用 Sentence Transformers 将文本转换为嵌入向量
   - 实现了 LangChain 的 Embeddings 接口

2. **向量存储 (VectorStore)**
   - 基于 FAISS 实现向量存储和检索
   - 支持文档增删和相似度搜索

3. **RAG 系统 (RAGSystem)**
   - 整合向量存储和语言模型
   - 实现检索增强生成功能

4. **文档处理器 (DocumentProcessor)**
   - 加载不同格式的文档（文本、PDF）
   - 将文档分割成适合嵌入的片段

## 三、搭建步骤详解

### 3.1 环境准备与依赖安装

首先需要安装必要的依赖包：
- `langchain` 及其相关包
- `sentence-transformers` 用于文本嵌入
- `faiss-cpu` 用于向量存储
- 其他辅助包如 `requests`、`python-dotenv` 等

```bash
pip install -r requirements.txt
```

### 3.2 实现本地嵌入模型

实现一个符合 LangChain Embeddings 接口的本地嵌入模型：

```python
from sentence_transformers import SentenceTransformer
from langchain_core.embeddings import Embeddings

class LocalEmbeddings(Embeddings):
    def __init__(self, model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        self.model = SentenceTransformer(model_name)
    
    def embed_documents(self, texts):
        # 将文档列表转换为嵌入向量
        embeddings = self.model.encode(texts)
        return embeddings.tolist()
    
    def embed_query(self, text):
        # 将查询文本转换为嵌入向量
        embedding = self.model.encode(text)
        return embedding.tolist()
```

### 3.3 构建向量数据库

使用 FAISS 作为后端存储引擎：

```python
from langchain_community.vectorstores import FAISS

# 创建向量存储
embedding_model = LocalEmbeddings()
vector_store = FAISS.from_documents(documents, embedding_model)

# 保存到本地
vector_store.save_local("path/to/faiss_index")

# 从本地加载
vector_store = FAISS.load_local("path/to/faiss_index", embedding_model)
```

### 3.4 连接自定义 LLM API

实现符合 LangChain BaseChatModel 接口的自定义 LLM 类：

```python
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, HumanMessage

class CustomLLM(BaseChatModel):
    def _generate(self, messages, stop=None, **kwargs):
        # 调用内部 API
        formatted_messages = self._format_messages(messages)
        response = self._call_api(formatted_messages)
        
        # 构造返回结果
        message = AIMessage(content=response)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])
```

### 3.5 构建 RAG 系统

将向量存储和语言模型整合为 RAG 系统：

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# 创建检索器
retriever = vector_store.as_retriever()

# 创建 RAG 链
prompt = ChatPromptTemplate.from_template("基于以下信息回答问题：\n{context}\n\n问题：{question}")
rag_chain = {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
```

## 四、系统工作流程解析

### 4.1 文档处理与嵌入

1. **文档加载**：从不同来源加载文档（文件、目录等）
2. **文档分割**：将文档分割为适合处理的片段
3. **生成嵌入**：使用本地嵌入模型将文档片段转换为向量
4. **存储向量**：将向量和原始文本存储在 FAISS 中

### 4.2 检索过程

1. **查询嵌入**：将用户查询转换为嵌入向量
2. **相似度搜索**：在向量数据库中查找最相似的文档
3. **检索文档**：返回相似度最高的几个文档

### 4.3 RAG 生成过程

1. **检索相关文档**：根据用户问题检索相关信息
2. **构建提示**：将检索到的文档和原始问题组合成提示
3. **生成回答**：调用语言模型生成最终回答

## 五、进阶优化方向

完成基础系统后，可以考虑以下优化方向：

### 5.1 嵌入模型优化

1. **选择更适合的模型**：
   - 对于中文文档，可以考虑使用 `shibing624/text2vec-base-chinese` 等针对中文优化的模型

2. **模型量化**：
   - 使用量化技术减小模型体积，加快推理速度

### 5.2 向量索引优化

1. **索引类型选择**：
   - 根据数据规模选择不同的 FAISS 索引类型
   - 小规模数据可用 `IndexFlatL2`，大规模数据可用 `IndexIVFFlat` 等

2. **混合搜索**：
   - 结合关键词搜索和向量搜索的混合策略

### 5.3 文档处理优化

1. **智能分块**：
   - 基于语义而非简单字符数进行文档分割

2. **元数据增强**：
   - 为文档添加更丰富的元数据，提高检索质量

### 5.4 检索策略优化

1. **重排序**：
   - 使用更复杂的重排序策略对初步检索结果进行优化

2. **多查询生成**：
   - 从原始问题生成多个查询，增加检索覆盖面

## 六、常见问题解答

### 6.1 为什么检索结果不相关？

可能原因：
- 嵌入模型不适合当前领域
- 文档分割不当导致上下文丢失
- 向量维度不足以表示复杂语义

解决方案：
- 尝试不同的嵌入模型
- 调整文档分割参数
- 优化检索策略，如增加检索数量然后进行重排序

### 6.2 系统运行缓慢？

可能原因：
- 嵌入模型过大
- 向量数据库索引效率低
- 文档过多导致检索变慢

解决方案：
- 使用更轻量级的嵌入模型或进行量化
- 优化 FAISS 索引类型
- 考虑分片或分布式存储

### 6.3 生成的回答不准确？

可能原因：
- 检索的文档不够相关
- 提示模板设计不佳
- 语言模型本身的局限

解决方案：
- 优化检索系统
- 改进提示工程
- 考虑使用更先进的语言模型

## 七、学习资源

1. **官方文档**：
   - [LangChain 文档](https://python.langchain.com/docs/get_started/introduction)
   - [FAISS 文档](https://github.com/facebookresearch/faiss/wiki)
   - [Sentence Transformers 文档](https://www.sbert.net/)

2. **教程**：
   - [LangChain 101 教程](https://python.langchain.com/docs/expression_language/cookbook)
   - [向量数据库实战指南](https://python.langchain.com/docs/modules/data_connection/vectorstores/)

3. **社区资源**：
   - [LangChain GitHub 讨论](https://github.com/langchain-ai/langchain/discussions)
   - [相关博客和案例](https://blog.langchain.dev/)
